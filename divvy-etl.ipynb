{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40cba206-333b-4879-a741-0bc24ec2367b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# DIVVY - ETL\n",
    "\n",
    "## Ingest the Data to the Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d1288d2-235f-40ba-9ccc-2befa55340d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, col, floor, months_between, expr\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db02db95-bce4-416f-ad10-ee58d5840745",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load CSV files into Spark DataFrames\n",
    "df_payment = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"false\").option(\"sep\", \",\").load(\"/FileStore/raw_data/payments.csv\")\n",
    "df_rider = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"false\").option(\"sep\", \",\").load(\"/FileStore/raw_data/riders.csv\")\n",
    "df_station = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"false\").option(\"sep\", \",\").load(\"/FileStore/raw_data/stations.csv\")\n",
    "df_trip = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"false\").option(\"sep\", \",\").load(\"/FileStore/raw_data/trips.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a8ed6ce-c278-4d20-9ab8-52e05c5f38e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write DataFrames to Delta Lake format for bronze layer storage\n",
    "df_payment.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/bronze/payment\")\n",
    "df_rider.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/bronze/rider\")\n",
    "df_station.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/bronze/station\")\n",
    "df_trip.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/bronze/trip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42031f24-90f9-45a8-8ff5-dc6dcd6b9ffa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Cleanse and Conform Data for the Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb45666d-0b31-492a-b1ac-4efde678dbf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "\n",
    "df_payment = (\n",
    "    df_payment.withColumnRenamed(\"_c0\", \"payment_id\")\n",
    "    .withColumnRenamed(\"_c1\", \"payment_date\")\n",
    "    .withColumnRenamed(\"_c2\", \"amount\")\n",
    "    .withColumnRenamed(\"_c3\", \"rider_id\")\n",
    ")\n",
    "\n",
    "df_rider = (\n",
    "    df_rider.withColumnRenamed(\"_c0\", \"rider_id\")\n",
    "    .withColumnRenamed(\"_c1\", \"first_name\")\n",
    "    .withColumnRenamed(\"_c2\", \"last_name\")\n",
    "    .withColumnRenamed(\"_c3\", \"address\")\n",
    "    .withColumnRenamed(\"_c4\", \"birthday\")\n",
    "    .withColumnRenamed(\"_c5\", \"account_start_date\")\n",
    "    .withColumnRenamed(\"_c6\", \"account_end_date\")\n",
    "    .withColumnRenamed(\"_c7\", \"is_member\")\n",
    ")\n",
    "\n",
    "df_station = (\n",
    "    df_station.withColumnRenamed(\"_c0\", \"station_id\")\n",
    "    .withColumnRenamed(\"_c1\", \"station_name\")\n",
    "    .withColumnRenamed(\"_c2\", \"latitude\")\n",
    "    .withColumnRenamed(\"_c3\", \"longitude\")\n",
    ")\n",
    "\n",
    "df_trip = (\n",
    "    df_trip.withColumnRenamed(\"_c0\", \"trip_id\")\n",
    "    .withColumnRenamed(\"_c1\", \"rideable_type\")\n",
    "    .withColumnRenamed(\"_c2\", \"start_at\")\n",
    "    .withColumnRenamed(\"_c3\", \"ended_at\")\n",
    "    .withColumnRenamed(\"_c4\", \"start_station_id\")\n",
    "    .withColumnRenamed(\"_c5\", \"end_station_id\")\n",
    "    .withColumnRenamed(\"_c6\", \"rider_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76c00a4c-6da8-4e1e-9542-1390bbddc776",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Deduplicate\n",
    "df_payment = df_payment.dropDuplicates(df_payment.columns)\n",
    "df_rider = df_rider.dropDuplicates(df_rider.columns)\n",
    "df_station = df_station.dropDuplicates(df_station.columns)\n",
    "df_trip = df_trip.dropDuplicates(df_trip.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "570b141f-4331-43b1-b86c-01321ae0735a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write DataFrames to Delta Lake format for silver layer storage\n",
    "df_payment.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/silver/payment\")\n",
    "df_rider.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/silver/rider\")\n",
    "df_station.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/silver/station\")\n",
    "df_trip.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/silver/trip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ff26e9d-d160-4c02-809a-b1df2e00d1af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Transform the data into the Star Schema for the Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95e2060e-5b45-4cb5-adba-c28e00e14fa4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add a new column 'trip_duration_minutes' to 'df_trip' with the difference in minutes between 'ended_at' and 'start_at'\n",
    "df_trip = df_trip.withColumn(\n",
    "    \"trip_duration_minutes\",\n",
    "    F.round((unix_timestamp(col(\"ended_at\")) - unix_timestamp(col(\"start_at\"))) / 60),\n",
    ")\n",
    "\n",
    "# Join 'df_trip' with 'df_rider' on 'rider_id' and add 'rider_age_at_trip' column to 'df_trip'\n",
    "df_trip = (\n",
    "    df_trip.join(df_rider.select(\"rider_id\", \"birthday\"), on=\"rider_id\", how=\"inner\")\n",
    "    .withColumn(\"trip_date\", col(\"start_at\").cast(\"date\"))\n",
    "    .withColumn(\n",
    "        \"rider_age_at_trip\",\n",
    "        floor(months_between(col(\"trip_date\"), col(\"birthday\")) / 12),\n",
    "    )\n",
    "    .drop(\"birthday\")  # Drop 'birthday' as no longer needed\n",
    ")\n",
    "\n",
    "# Add a new column 'rider_age_at_signup' to 'df_rider' to calculate the rider's age at the time of signup\n",
    "df_rider = df_rider.withColumn(\n",
    "    \"rider_age_at_signup\",\n",
    "    floor(months_between(col(\"account_start_date\"), col(\"birthday\")) / 12),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e83d675-7c16-4e96-bcc0-dbb1206fdee1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Date Dimension DataFrame\n",
    "\n",
    "# Get minimum and maximum dates from 'df_trip' and 'df_payment'\n",
    "trip_min_date, trip_max_date = df_trip.agg(\n",
    "    F.min(\"trip_date\"), F.max(\"trip_date\")\n",
    ").first()\n",
    "payment_min_date, payment_max_date = df_payment.agg(\n",
    "    F.min(\"payment_date\"), F.max(\"payment_date\")\n",
    ").first()\n",
    "\n",
    "# Determine overall min and max dates\n",
    "min_date = min(trip_min_date, payment_min_date)\n",
    "max_date = max(trip_max_date, payment_max_date)\n",
    "\n",
    "# Convert datetime.date objects to strings in 'YYYY-MM-DD' format\n",
    "min_date_str = min_date.strftime(\"%Y-%m-%d\")\n",
    "max_date_str = max_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Generate date range DataFrame\n",
    "df_date = (\n",
    "    spark.range(0, (max_date - min_date).days + 1, 1)\n",
    "    .withColumn(\"id\", col(\"id\").cast(\"int\"))\n",
    "    .withColumn(\"date\", expr(f\"date_add('{min_date_str}', id)\"))\n",
    ")\n",
    "\n",
    "# Add other date attributes\n",
    "df_date = (\n",
    "    df_date.withColumn(\"year\", F.year(col(\"date\")))\n",
    "    .withColumn(\"quarter\", F.quarter(col(\"date\")))\n",
    "    .withColumn(\"month\", F.month(col(\"date\")))\n",
    "    .withColumn(\"day\", F.dayofmonth(col(\"date\")))\n",
    "    .withColumn(\"week\", F.weekofyear(col(\"date\")))\n",
    "    .withColumn(\"is_weekend\", F.dayofweek(col(\"date\")).isin([1, 7]).cast(\"boolean\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45a18bb1-51a4-4b42-a813-d4bd1caae801",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write DataFrames to Delta Lake format, and save as a Delta tables for gold layer\n",
    "df_date.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_dim_date\")\n",
    "df_station.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_dim_station\")\n",
    "df_rider.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_dim_rider\")\n",
    "df_trip.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_fact_trip\")\n",
    "df_payment.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_fact_payment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "496babd0-7355-4b62-8acb-ebca41c943d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold_dim_date:\n+---+----------+----+-------+-----+---+----+----------+\n| id|      date|year|quarter|month|day|week|is_weekend|\n+---+----------+----+-------+-----+---+----+----------+\n|  0|2013-02-01|2013|      1|    2|  1|   5|     false|\n|  1|2013-02-02|2013|      1|    2|  2|   5|      true|\n|  2|2013-02-03|2013|      1|    2|  3|   5|      true|\n|  3|2013-02-04|2013|      1|    2|  4|   6|     false|\n|  4|2013-02-05|2013|      1|    2|  5|   6|     false|\n+---+----------+----+-------+-----+---+----+----------+\nonly showing top 5 rows\n\ngold_dim_station:\n+------------+--------------------+------------------+------------------+\n|  station_id|        station_name|          latitude|         longitude|\n+------------+--------------------+------------------+------------------+\n|       15550|Canal St & Taylor St|         41.870257|-87.63947399999999|\n|TA1307000150|Pine Grove Ave & ...| 41.94947274088333|-87.64645278453827|\n|TA1307000156|Lincoln Ave & Sun...|         41.963004|        -87.684781|\n|        E006|    Central St Metra|         42.063598|         -87.69873|\n|KA1503000055|Halsted & 63rd - ...|41.779381142279995|    -87.6446208145|\n+------------+--------------------+------------------+------------------+\nonly showing top 5 rows\n\ngold_dim_rider:\n+--------+----------+---------+--------------------+----------+------------------+----------------+---------+-------------------+\n|rider_id|first_name|last_name|             address|  birthday|account_start_date|account_end_date|is_member|rider_age_at_signup|\n+--------+----------+---------+--------------------+----------+------------------+----------------+---------+-------------------+\n|    1258|    Justin| Williams|3700 Salazar Pass...|1990-08-27|        2021-05-23|            NULL|     true|                 30|\n|    1649|     Jacob|  Rosales|38480 Kelsey Stra...|1994-12-12|        2014-01-29|            NULL|    false|                 19|\n|    1761|   Maurice|  Griffin|  2683 David Station|1972-03-10|        2015-12-04|      2021-10-01|     true|                 43|\n|    2003|   Whitney|    Lewis|065 Holmes Crest ...|1985-02-06|        2020-10-18|            NULL|    false|                 35|\n|    2246|   Rebecca|   Miller|  3003 Walker Cliffs|1984-11-08|        2017-09-22|      2021-12-01|     true|                 32|\n+--------+----------+---------+--------------------+----------+------------------+----------------+---------+-------------------+\nonly showing top 5 rows\n\ngold_fact_trip:\n+--------+----------------+-------------+-------------------+-------------------+----------------+--------------+---------------------+----------+-----------------+\n|rider_id|         trip_id|rideable_type|           start_at|           ended_at|start_station_id|end_station_id|trip_duration_minutes| trip_date|rider_age_at_trip|\n+--------+----------------+-------------+-------------------+-------------------+----------------+--------------+---------------------+----------+-----------------+\n|   45293|8555029D85CDA86C| classic_bike|2021-02-02 11:20:31|2021-02-02 11:28:43|    KA1504000133|  TA1305000029|                  8.0|2021-02-02|               19|\n|   63427|8DCE018433065D46| classic_bike|2021-02-27 16:04:21|2021-02-27 16:20:41|    TA1306000032|         13276|                 16.0|2021-02-27|               33|\n|   55641|B8A3C5476D8051B7|  docked_bike|2021-02-27 21:16:58|2021-02-27 21:42:33|           15667|  TA1307000166|                 26.0|2021-02-27|               18|\n|   47734|5BFD74ED7D210CB3| classic_bike|2021-02-23 16:52:27|2021-02-23 17:18:22|    KA1503000041|  KA1503000041|                 26.0|2021-02-23|               23|\n|   30150|21C4F6436A5CFFB6| classic_bike|2021-02-04 16:39:49|2021-02-04 16:50:45|    TA1305000032|         13427|                 11.0|2021-02-04|               16|\n+--------+----------------+-------------+-------------------+-------------------+----------------+--------------+---------------------+----------+-----------------+\nonly showing top 5 rows\n\ngold_fact_payment:\n+----------+------------+------+--------+\n|payment_id|payment_date|amount|rider_id|\n+----------+------------+------+--------+\n|       165|  2019-02-01| 15.47|    1007|\n|       342|  2019-09-01|   9.0|    1014|\n|       398|  2019-10-01|   9.0|    1015|\n|       543|  2020-04-01|   9.0|    1022|\n|       554|  2021-03-01|   9.0|    1022|\n+----------+------------+------+--------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 rows from each Delta table in the gold layer for verification\n",
    "\n",
    "print(\"gold_dim_date:\")\n",
    "spark.read.table(\"gold_dim_date\").show(5)\n",
    "\n",
    "print(\"gold_dim_station:\")\n",
    "spark.read.table(\"gold_dim_station\").show(5)\n",
    "\n",
    "print(\"gold_dim_rider:\")\n",
    "spark.read.table(\"gold_dim_rider\").show(5)\n",
    "\n",
    "print(\"gold_fact_trip:\")\n",
    "spark.read.table(\"gold_fact_trip\").show(5)\n",
    "\n",
    "print(\"gold_fact_payment:\")\n",
    "spark.read.table(\"gold_fact_payment\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef761e7e-1190-4f4c-922a-dcb5aa30dc23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Uncomment if you need to drop Delta tables\n",
    "--DROP TABLE IF EXISTS gold_dim_date;\n",
    "--DROP TABLE IF EXISTS gold_dim_station;\n",
    "--DROP TABLE IF EXISTS gold_dim_rider;\n",
    "--DROP TABLE IF EXISTS gold_fact_trip;\n",
    "--DROP TABLE IF EXISTS gold_fact_payment;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d3a7f71-29ef-489a-9b9e-b6ee71a46e37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment if you need to remove the entire '/delta' directory and all its contents\n",
    "#dbutils.fs.rm(\"/delta\", recurse=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "divvy-etl",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
